{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_conversion",
   "metadata": {},
   "source": [
    "# PDF to Markdown Conversion\n",
    "\n",
    "This notebook compares different methods for converting complex PDF contracts into Markdown.\n",
    "\n",
    "**Methods Evaluated:**\n",
    "1. **MarkItDown**: Microsoft's tool for converting various formats to text.\n",
    "2. **Dolphin / GOT-OCR2.0**: A VLM-based approach for high-fidelity OCR.\n",
    "3. **Docling**: A specialized document conversion library (IBM), including custom pipeline tweaks for table structure and heading detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37534446",
   "metadata": {},
   "source": [
    "#### Markitdown Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dab962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from markitdown import MarkItDown\n",
    "import os\n",
    "import glob\n",
    "\n",
    "md = MarkItDown()\n",
    "\n",
    "# Input folder (where your PDFs are stored)\n",
    "input_folder = \"/Users/swathi.gnanasekar/Documents/Vista_Vu_Project/Phase 1/Contracts_Initial\"\n",
    "\n",
    "# Output folder (where parsed files will go)\n",
    "output_folder = \"/Users/swathi.gnanasekar/Documents/Vista_Vu_Project/Phase 1/Parsed Contracts\"\n",
    "\n",
    "# Make sure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Find all PDFs recursively\n",
    "files = glob.glob(os.path.join(input_folder, \"**\", \"*.pdf\"), recursive=True)\n",
    "files += glob.glob(os.path.join(input_folder, \"**\", \"*.PDF\"), recursive=True)\n",
    "\n",
    "parsed_docs = {}\n",
    "\n",
    "for file in files:\n",
    "    result = md.convert(file)\n",
    "\n",
    "    parsed_docs[file] = {\n",
    "        \"text\": result.text_content,\n",
    "        \"markdown\": result.markdown\n",
    "    }\n",
    "\n",
    "    # Get base filename only (no folders, no extension)\n",
    "    base = os.path.splitext(os.path.basename(file))[0]\n",
    "\n",
    "    # Save outputs into Parsed Contracts\n",
    "    with open(os.path.join(output_folder, f\"{base}_parsed.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(parsed_docs[file][\"text\"])\n",
    "\n",
    "    with open(os.path.join(output_folder, f\"{base}_parsed.md\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(parsed_docs[file][\"markdown\"])\n",
    "\n",
    "print(f\"Processed {len(files)} files. Results saved in {output_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0007bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64dfff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58758b95",
   "metadata": {},
   "source": [
    "### Dolphin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295dc558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# change to dolphin directory\n",
    "os.chdir(\"/Users/swathi.gnanasekar/Documents/Vista_Vu_Project/Dolphin\")\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "input_folder = \"/Users/swathi.gnanasekar/Documents/Vista_Vu_Project/Phase 1/Contracts_Initial\"\n",
    "output_folder = \"/Users/swathi.gnanasekar/Documents/Vista_Vu_Project/Phase 1/Dolphin_Parsed\"\n",
    "model_path = \"/Users/swathi.gnanasekar/Documents/Vista_Vu_Project/Dolphin/hf_model\"  # adjust if different\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Find all PDFs (case insensitive)\n",
    "files = glob.glob(os.path.join(input_folder, \"**\", \"*.pdf\"), recursive=True)\n",
    "files += glob.glob(os.path.join(input_folder, \"**\", \"*.PDF\"), recursive=True)\n",
    "\n",
    "print(f\"Found {len(files)} PDF files to process.\")\n",
    "\n",
    "for file in files:\n",
    "    print(f\"Processing {file} ...\")\n",
    "    # Run Dolphin per file\n",
    "    subprocess.run([\n",
    "        \"python\", \"demo_page_hf.py\",\n",
    "        \"--model_path\", model_path,\n",
    "        \"--input_path\", file,\n",
    "        \"--save_dir\", output_folder\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6970da",
   "metadata": {},
   "source": [
    "### Docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66dc3991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.pipeline.vlm_pipeline import VlmPipeline\n",
    "from docling.datamodel.pipeline_options import VlmPipelineOptions\n",
    "from docling.datamodel import vlm_model_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a224a276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AcceleratorDevice',\n",
       " 'AnyUrl',\n",
       " 'ApiVlmOptions',\n",
       " 'DOLPHIN_TRANSFORMERS',\n",
       " 'Enum',\n",
       " 'GEMMA3_12B_MLX',\n",
       " 'GEMMA3_27B_MLX',\n",
       " 'GOT2_TRANSFORMERS',\n",
       " 'GRANITEDOCLING_MLX',\n",
       " 'GRANITEDOCLING_TRANSFORMERS',\n",
       " 'GRANITEDOCLING_VLLM',\n",
       " 'GRANITE_VISION_OLLAMA',\n",
       " 'GRANITE_VISION_TRANSFORMERS',\n",
       " 'GRANITE_VISION_VLLM',\n",
       " 'InferenceFramework',\n",
       " 'InlineVlmOptions',\n",
       " 'NU_EXTRACT_2B_TRANSFORMERS',\n",
       " 'PHI4_TRANSFORMERS',\n",
       " 'PIXTRAL_12B_MLX',\n",
       " 'PIXTRAL_12B_TRANSFORMERS',\n",
       " 'QWEN25_VL_3B_MLX',\n",
       " 'ResponseFormat',\n",
       " 'SMOLDOCLING_MLX',\n",
       " 'SMOLDOCLING_TRANSFORMERS',\n",
       " 'SMOLDOCLING_VLLM',\n",
       " 'SMOLVLM256_MLX',\n",
       " 'SMOLVLM256_TRANSFORMERS',\n",
       " 'SMOLVLM256_VLLM',\n",
       " 'TransformersModelType',\n",
       " 'TransformersPromptStyle',\n",
       " 'VlmModelType',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_log',\n",
       " 'logging']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(vlm_model_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ca27dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 10:54:55,829 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-03 10:54:55,907 - INFO - Going to convert document batch...\n",
      "2025-11-03 10:54:55,908 - INFO - Initializing pipeline for VlmPipeline with options hash 0b46d09deed704fae432713af4b78bcd\n",
      "2025-11-03 10:54:55,941 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-11-03 10:54:55,943 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-11-03 10:55:07,033 - INFO - Processing document NMA-Alberta-Province.pdf\n",
      "2025-11-03 11:07:27,035 - INFO - Finished converting document NMA-Alberta-Province.pdf in 751.23 sec.\n",
      "2025-11-03 11:07:27,290 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-03 11:07:27,307 - INFO - Going to convert document batch...\n",
      "2025-11-03 11:07:27,308 - INFO - Processing document NWR Edmonton Exchanger.pdf\n",
      "2025-11-03 11:23:49,273 - INFO - Finished converting document NWR Edmonton Exchanger.pdf in 981.94 sec.\n",
      "2025-11-03 11:23:49,602 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-03 11:23:49,619 - INFO - Going to convert document batch...\n",
      "2025-11-03 11:23:49,620 - INFO - Processing document C1000776 CO#8 Combined - Labour Rates update June 1, 2025.pdf\n",
      "2025-11-03 11:33:46,346 - INFO - Finished converting document C1000776 CO#8 Combined - Labour Rates update June 1, 2025.pdf in 596.76 sec.\n",
      "2025-11-03 11:33:46,883 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-03 11:33:46,893 - INFO - Going to convert document batch...\n",
      "2025-11-03 11:33:46,894 - INFO - Processing document C1000776 CO#6 Combined - Labour and Equipment Rates and Extension to Feb 21 2027.pdf\n",
      "2025-11-03 11:36:53,799 - INFO - Finished converting document C1000776 CO#6 Combined - Labour and Equipment Rates and Extension to Feb 21 2027.pdf in 186.93 sec.\n",
      "2025-11-03 11:36:53,978 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-03 11:36:53,984 - INFO - Going to convert document batch...\n",
      "2025-11-03 11:36:53,985 - INFO - Processing document Boilermakers Collective-Agreement.pdf\n",
      "2025-11-03 11:52:39,987 - INFO - Finished converting document Boilermakers Collective-Agreement.pdf in 946.04 sec.\n",
      "2025-11-03 11:52:40,434 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-03 11:52:40,439 - INFO - Going to convert document batch...\n",
      "2025-11-03 11:52:40,439 - INFO - Processing document Pipefitters-Collective-Agreement_April_2025-CLEAN_updated_May-22-onfile.pdf\n",
      "2025-11-03 12:06:20,990 - INFO - Finished converting document Pipefitters-Collective-Agreement_April_2025-CLEAN_updated_May-22-onfile.pdf in 820.58 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6 PDFs with Docling.\n"
     ]
    }
   ],
   "source": [
    "pipeline_options = VlmPipelineOptions(\n",
    "    vlm_options = vlm_model_specs.GRANITEDOCLING_MLX  # or a model appropriate to your hardware \n",
    ")\n",
    "\n",
    "converter = DocumentConverter(\n",
    "    format_options = {\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_cls = VlmPipeline,\n",
    "            pipeline_options = pipeline_options\n",
    "        )\n",
    "    }\n",
    ")\n",
    "input_folder = \"/Users/swathi.gnanasekar/Documents/Vista_Vu_Project/Phase 1/Contracts_Initial\"\n",
    "output_folder = \"/Users/swathi.gnanasekar/Documents/Vista_Vu_Project/Phase 1/Docling_Vlm_parsed\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "pdf_files = glob.glob(os.path.join(input_folder, \"**\", \"*.pdf\"), recursive=True)\n",
    "\n",
    "for filepath in pdf_files:\n",
    "    result = converter.convert(filepath)\n",
    "    base = os.path.splitext(os.path.basename(filepath))[0]\n",
    "\n",
    "    # Save Markdown\n",
    "    with open(os.path.join(output_folder, f\"{base}.md\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(result.document.export_to_markdown())\n",
    "\n",
    "    # Save JSON\n",
    "    with open(os.path.join(output_folder, f\"{base}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        import json\n",
    "        json.dump(result.document.export_to_dict(), f, indent=2)\n",
    "\n",
    "print(f\"Processed {len(pdf_files)} PDFs with Docling.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3beef9f",
   "metadata": {},
   "source": [
    "### Dolphin_New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3385a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "\n",
    "input_folder = \"/Users/swathi.gnanasekar/Documents/Vista_Vu_Project/Phase 1/Contracts_Initial\"\n",
    "files = glob.glob(os.path.join(input_folder, \"**\", \"*.pdf\"), recursive=True)\n",
    "files += glob.glob(os.path.join(input_folder, \"**\", \"*.PDF\"), recursive=True)\n",
    "print(\"PDFs found:\", len(files))\n",
    "print(files[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "input_folder = \"/Users/swathi.gnanasekar/Documents/Vista_Vu_Project/Phase 1/Contracts_Initial\"\n",
    "output_folder = \"/Users/swathi.gnanasekar/Documents/Vista_Vu_Project/Phase 1/Dolphin_New\"\n",
    "model_name = \"ucaslcl/GOT-OCR2_0\"\n",
    "device = \"cpu\"  # force CPU mode\n",
    "dpi = 150       # lower DPI = faster rendering\n",
    "# ===================================\n",
    "\n",
    "\n",
    "# ========== LOAD MODEL ==========\n",
    "print(\"Loading GOT-OCR2.0 model (CPU mode)...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded.\\n\")\n",
    "# ===================================\n",
    "\n",
    "\n",
    "def pdf_to_images(pdf_path, dpi=150):\n",
    "    \"\"\"Convert PDF pages to PIL Images\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages = []\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        zoom = dpi / 72\n",
    "        mat = fitz.Matrix(zoom, zoom)\n",
    "        pix = page.get_pixmap(matrix=mat)\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        pages.append((page_num, img))\n",
    "    doc.close()\n",
    "    return pages\n",
    "\n",
    "\n",
    "def process_pdf(pdf_path):\n",
    "    \"\"\"Convert PDF pages to markdown using GOT-OCR2.0\"\"\"\n",
    "    print(f\"Processing {os.path.basename(pdf_path)}\")\n",
    "    pages = pdf_to_images(pdf_path, dpi)\n",
    "    all_md = []\n",
    "\n",
    "    for page_num, img in tqdm(pages, desc=\"Pages\", leave=False):\n",
    "        temp_path = f\"temp_page_{page_num}.png\"\n",
    "        img.save(temp_path)\n",
    "\n",
    "        try:\n",
    "            # Force model to use CPU by setting device_map\n",
    "            with torch.no_grad():\n",
    "                md_text = model.chat(\n",
    "                    tokenizer,\n",
    "                    temp_path,\n",
    "                    ocr_type=\"format\",  # markdown-like\n",
    "                    device=device\n",
    "                )\n",
    "            all_md.append(f\"## Page {page_num+1}\\n\\n{md_text}\\n\\n---\\n\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error on page {page_num}: {e}\")\n",
    "\n",
    "        finally:\n",
    "            os.remove(temp_path)\n",
    "\n",
    "    return \"\".join(all_md)\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    pdf_files = [\n",
    "        os.path.join(root, f)\n",
    "        for root, _, files in os.walk(input_folder)\n",
    "        for f in files if f.lower().endswith(\".pdf\")\n",
    "    ]\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(\"⚠️ No PDFs found.\")\n",
    "        return\n",
    "\n",
    "    for pdf in pdf_files:\n",
    "        md_text = process_pdf(pdf)\n",
    "        out_name = os.path.splitext(os.path.basename(pdf))[0] + \".md\"\n",
    "        out_path = os.path.join(output_folder, out_name)\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(md_text)\n",
    "        print(f\"✅ Saved: {out_path}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bd3a10",
   "metadata": {},
   "source": [
    "### Appendix f issue fix with tweak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4533fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 12:08:40,020 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-03 12:08:40,026 - INFO - Going to convert document batch...\n",
      "2025-11-03 12:08:40,027 - INFO - Initializing pipeline for StandardPdfPipeline with options hash f9730ffaa6e7f8d4fb0c98c8df3f18cb\n",
      "2025-11-03 12:08:40,077 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-11-03 12:08:40,085 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-11-03 12:08:46,424 - INFO - Auto OCR model selected ocrmac.\n",
      "2025-11-03 12:08:46,434 - INFO - Accelerator device: 'mps'\n",
      "2025-11-03 12:08:54,317 - INFO - Accelerator device: 'mps'\n",
      "2025-11-03 12:08:55,021 - INFO - Processing document NMA-Alberta-Province.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(pages)=1, 0-0\n",
      "len(valid_pages)=1\n",
      "len(valid_page_images)=1\n",
      "len(pages)=4, 1-4\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 5-8\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 9-12\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 13-16\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 17-20\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 21-24\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 25-28\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 29-32\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 33-36\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 37-40\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=2, 41-42\n",
      "len(valid_pages)=2\n",
      "len(valid_page_images)=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 12:09:19,005 - INFO - Finished converting document NMA-Alberta-Province.pdf in 38.99 sec.\n",
      "2025-11-03 12:09:19,450 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-03 12:09:19,472 - INFO - Going to convert document batch...\n",
      "2025-11-03 12:09:19,473 - INFO - Processing document NWR Edmonton Exchanger.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(pages)=1, 0-0\n",
      "len(valid_pages)=1\n",
      "len(valid_page_images)=1\n",
      "len(pages)=3, 1-3\n",
      "len(valid_pages)=3\n",
      "len(valid_page_images)=3\n",
      "len(pages)=4, 4-7\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 8-11\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 12-15\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 16-19\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 20-23\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 24-27\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 28-31\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 32-35\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 36-39\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 40-43\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 44-47\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 48-51\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 52-55\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 56-59\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 60-63\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 64-67\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 68-71\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=3, 72-74\n",
      "len(valid_pages)=3\n",
      "len(valid_page_images)=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 12:11:09,556 - INFO - Finished converting document NWR Edmonton Exchanger.pdf in 110.11 sec.\n",
      "2025-11-03 12:11:10,333 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-03 12:11:10,362 - INFO - Going to convert document batch...\n",
      "2025-11-03 12:11:10,362 - INFO - Processing document C1000776 CO#8 Combined - Labour Rates update June 1, 2025.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(pages)=1, 0-0\n",
      "len(valid_pages)=1\n",
      "len(valid_page_images)=1\n",
      "len(pages)=1, 1-1\n",
      "len(valid_pages)=1\n",
      "len(valid_page_images)=1\n",
      "len(pages)=4, 2-5\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 6-9\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=1, 10-10\n",
      "len(valid_pages)=1\n",
      "len(valid_page_images)=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 12:12:16,122 - INFO - Finished converting document C1000776 CO#8 Combined - Labour Rates update June 1, 2025.pdf in 65.79 sec.\n",
      "2025-11-03 12:12:17,245 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-03 12:12:17,254 - INFO - Going to convert document batch...\n",
      "2025-11-03 12:12:17,254 - INFO - Processing document C1000776 CO#6 Combined - Labour and Equipment Rates and Extension to Feb 21 2027.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(pages)=1, 0-0\n",
      "len(valid_pages)=1\n",
      "len(valid_page_images)=1\n",
      "len(pages)=4, 1-4\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 5-8\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 9-12\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=1, 13-13\n",
      "len(valid_pages)=1\n",
      "len(valid_page_images)=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 12:13:43,038 - INFO - Finished converting document C1000776 CO#6 Combined - Labour and Equipment Rates and Extension to Feb 21 2027.pdf in 85.80 sec.\n",
      "2025-11-03 12:13:44,303 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-03 12:13:44,312 - INFO - Going to convert document batch...\n",
      "2025-11-03 12:13:44,312 - INFO - Processing document Boilermakers Collective-Agreement.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(pages)=1, 0-0\n",
      "len(valid_pages)=1\n",
      "len(valid_page_images)=1\n",
      "len(pages)=4, 1-4\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 5-8\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 9-12\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 13-16\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 17-20\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 21-24\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 25-28\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 29-32\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 33-36\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 37-40\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 41-44\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 45-48\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 49-52\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 53-56\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 57-60\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 61-64\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 65-68\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 69-72\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=2, 73-74\n",
      "len(valid_pages)=2\n",
      "len(valid_page_images)=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 12:14:17,598 - INFO - Finished converting document Boilermakers Collective-Agreement.pdf in 33.30 sec.\n",
      "2025-11-03 12:14:17,890 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-03 12:14:17,896 - INFO - Going to convert document batch...\n",
      "2025-11-03 12:14:17,896 - INFO - Processing document Pipefitters-Collective-Agreement_April_2025-CLEAN_updated_May-22-onfile.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(pages)=1, 0-0\n",
      "len(valid_pages)=1\n",
      "len(valid_page_images)=1\n",
      "len(pages)=3, 1-3\n",
      "len(valid_pages)=3\n",
      "len(valid_page_images)=3\n",
      "len(pages)=4, 4-7\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 8-11\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 12-15\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 16-19\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 20-23\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 24-27\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 28-31\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 32-35\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 36-39\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 40-43\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 44-47\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 48-51\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 52-55\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 56-59\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 60-63\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n",
      "len(pages)=4, 64-67\n",
      "len(valid_pages)=4\n",
      "len(valid_page_images)=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 12:15:36,114 - INFO - Finished converting document Pipefitters-Collective-Agreement_April_2025-CLEAN_updated_May-22-onfile.pdf in 78.23 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6 PDFs with adjusted pipeline.\n"
     ]
    }
   ],
   "source": [
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, TableFormerMode\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "import os, glob, json\n",
    "\n",
    "input_folder = \"/Users/swathi.gnanasekar/Documents/Vista_Vu_Project/Phase 1/Contracts_Initial\"\n",
    "output_folder = \"/Users/swathi.gnanasekar/Documents/Vista_Vu_Project/Phase 1/Docling_Tweak\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Configure pipeline options\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.do_picture_classification = False  # disable picture classification\n",
    "pipeline_options.generate_page_images = False        # optionally disable generating full page images\n",
    "# Table processing adjustments\n",
    "pipeline_options.do_table_structure = True\n",
    "pipeline_options.table_structure_options.mode = TableFormerMode.ACCURATE\n",
    "\n",
    "# Create converter with these options\n",
    "converter = DocumentConverter(\n",
    "    format_options = {\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Process files\n",
    "pdf_files = glob.glob(os.path.join(input_folder, \"**\", \"*.pdf\"), recursive=True)\n",
    "for filepath in pdf_files:\n",
    "    result = converter.convert(filepath)\n",
    "    base = os.path.splitext(os.path.basename(filepath))[0]\n",
    "\n",
    "    # Post-processing: detect header banner treated as picture and convert\n",
    "    doc = result.document\n",
    "    for pic in list(doc.pictures):  # iterate through PictureItem list\n",
    "        text = getattr(pic, \"ocr_text\", None)\n",
    "        if text and text.strip().upper().startswith(\"APPENDIX\"):\n",
    "            # Convert this picture item into a heading\n",
    "            # Simplified example: remove from pictures, add to texts as heading\n",
    "            doc.pictures.remove(pic)\n",
    "            doc.texts.append(\n",
    "                type(pic)(  # using class of picture? adjust accordingly\n",
    "                    text=text,\n",
    "                    label=\"HEADING\",\n",
    "                    heading_level=2,\n",
    "                    bbox=pic.bbox,\n",
    "                    parent=pic.parent,\n",
    "                    provenance=pic.provenance\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Save Markdown\n",
    "    with open(os.path.join(output_folder, f\"{base}.md\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(doc.export_to_markdown())\n",
    "    # Save JSON\n",
    "    with open(os.path.join(output_folder, f\"{base}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(doc.export_to_dict(), f, indent=2)\n",
    "\n",
    "print(f\"Processed {len(pdf_files)} PDFs with adjusted pipeline.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vistavu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
