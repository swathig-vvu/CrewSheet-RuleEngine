{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d45f43b",
   "metadata": {},
   "source": [
    "# Chunk Categorization\n",
    "\n",
    "This notebook classifies text chunks into specific contract categories using LLMs.\n",
    "\n",
    "**Workflow:**\n",
    "1. **Load Chunks**: Reads chunked text files.\n",
    "2. **Langfuse Integration**: Manages prompts and traces execution.\n",
    "3. **Gemini Classification**: Uses Google Gemini 2.5 Pro to classify each chunk into:\n",
    "   - **Contract Section Type** (e.g., COMPENSATION)\n",
    "   - **Second Level** (e.g., PAYABLE_ENTITLEMENT)\n",
    "   - **Third Level** (e.g., LABOUR)\n",
    "4. **Output**: Saves the classification results to JSON for database loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1a3386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 3: Batch classify chunk files with Gemini 2.5 Pro via LangChain + Langfuse\n",
    "# ==============================================================================\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Langfuse Imports ---\n",
    "from langfuse import Langfuse\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9221648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGFUSE_SECRET_KEY = \"sk-lf-47e1f03b-e7b8-4d88-9676-5b499727c074\"\n",
    "LANGFUSE_PUBLIC_KEY = \"pk-lf-ae5d3d74-de3b-498c-84bf-4ed6b9f43c2f\"\n",
    "LANGFUSE_BASE_URL = \"http://localhost:3000\"\n",
    "GOOGLE_API_KEY = \"AIzaSyC9J2RpqaNZnZ3XMSh4XnR1JT44THF4wE4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e684e734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AIzaSyC9J2RpqaNZnZ3XMSh4XnR1JT44THF4wE4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = LANGFUSE_SECRET_KEY \n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = LANGFUSE_PUBLIC_KEY\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"http://localhost:3000\" \n",
    "os.environ.setdefault(\"GOOGLE_API_KEY\", GOOGLE_API_KEY) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f822fcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767120014.726540  373686 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 01__ARTICLE_100_-PURPOSE.txt\n",
      "Processing: 02__ARTICLE_200_-RECOGNITION_AND_CRAFT_JURISDICTION.txt\n",
      "Processing: 03__ARTICLE_300_-MANAGEMENT_RIGHTS.txt\n",
      "Processing: 04__ARTICLE_400_-UNION_SECURITY_AND_DUES_COLLECTION.txt\n",
      "Processing: 05__ARTICLE_500_-NO_STRIKES_OR_LOCKOUTS.txt\n",
      "Processing: 06__ARTICLE_600_-WORKING_CONDITIONS_SAFEFTY_MEASURES_HEALTH_AND_SANITATION.txt\n",
      "Processing: 07__ARTICLE_700_-WELDING_TESTS.txt\n",
      "Processing: 08__ARTICLE_800_-ACCESS_TO_JOBS.txt\n",
      "Processing: 09__ARTICLE_900_-JOB_STEWARDS.txt\n",
      "Processing: 10__ARTICLE_1000_-GRIEVANCE_PROCEDURE.txt\n",
      "Processing: 11__ARTICLE_1100_-HOURS_OF_WORK.txt\n",
      "Processing: 12__ARTICLE_1200_-SHIFT_WORK.txt\n",
      "Processing: 13__ARTICLE_1300_-OVERTIMEOVERTIME_MEAL_BREAKS.txt\n",
      "Processing: 14__ARTICLE_1400_-RECOGNIZED_HOLIDAYS.txt\n",
      "Processing: 15__ARTICLE_1500_-WAITING_AND_REPORTING_TIME.txt\n",
      "Processing: 16__ARTICLE_1600_-TRAVEL_AND_SUBSISTENCE.txt\n",
      "Processing: 17__ARTICLE_1700_-VACATION_WITH_PAY.txt\n",
      "Processing: 18__ARTICLE_1800_-PAY_DAY.txt\n",
      "Processing: 19__ARTICLE_1900_-WAGES.txt\n",
      "Processing: 20__ARTICLE_2000_-PARTICIPATION_AGREEMENT.txt\n",
      "Processing: 21__ARTICLE_2100_-TANK_WORK_EMPLOYEES.txt\n",
      "Processing: 22__ARTICLE_2200_-ADMINISTRATION_OF_AGREEMENT.txt\n",
      "Processing: 23__ARTICLE_2300_-IMPLEMENTATION_DURATION_AND_RENEWAL_OF_AGREEMENT.txt\n",
      "Processing: 24__ARTICLE_2400_-SUBMISSION_OF_DUES_AND_OTHER_CONTRIBUTIONS.txt\n",
      "Processing: 25__ARTICLE_2500_-ENABLING_CLAUSE.txt\n",
      "Processing: 26_Tank_Work_Employers_Letter_Referred_to_In_Article_2100.txt\n",
      "Processing: 27__LETTER_2.txt\n",
      "Processing: 28__LETTER_3.txt\n",
      "Processing: 29__LETTER_4.txt\n",
      "Processing: 30__LETTER_5.txt\n",
      "Processing: 31__LETTER_6.txt\n",
      "Processing: 32__LETTER_7.txt\n",
      "Processing: 33_Letter_8.txt\n",
      "Processing: 34__APPENDIX_A.txt\n",
      "Processing: 35__APPENDIX_B.txt\n",
      "Processing: 36__APPENDIX_C.txt\n",
      "Processing: 37__APPENDIX_D_-SIGNATORY_EMPLOYERS.txt\n",
      "Processing: 38__ADDRESSES.txt\n",
      "Processing: 01__ARTICLE_1000_RECOGNITION.txt\n",
      "Processing: 02__ARTICLE_2000_THE_COMPANY_AND_THE_UNIONS.txt\n",
      "Processing: 03__ARTICLE_3000_UNION_SECURITY.txt\n",
      "Processing: 04__ARTICLE_4000_SCOPE_OF_WORK.txt\n",
      "Processing: 05__ARTICLE_5000_DEFINITIONS.txt\n",
      "Processing: 06__ARTICLE_6000_GRIEVANCE_PROCEDURE.txt\n",
      "Processing: 07__ARTICLE_7000_UNION_REPRESENTATION.txt\n",
      "Processing: 08__ARTICLE_8000_STEWARDS.txt\n",
      "Processing: 09__ARTICLE_9000_WAGES.txt\n",
      "Processing: 10__ARTICLE_10000_BENEFITS_AND_OTHER_MONETARY_FUNDS.txt\n",
      "Processing: 11__ARTICLE_11000_STATUTORY_HOLIDAYS.txt\n",
      "Processing: 12__ARTICLE_12000_MINIMUM_PAY_AND_REPORTING_TIME.txt\n",
      "Processing: 13__ARTICLE_13000_TRAVEL_AND_SUBSISTENCE.txt\n",
      "Processing: 14__ARTICLE_14000_WORK_BREAKS.txt\n",
      "Processing: 15__ARTICLE_15000_WORK_HOURS_PER_DAY_OVERTIME_AND_OVERTIME_MEAL_BREAKS.txt\n",
      "Processing: 16__ARTICLE_16000_SAFETY.txt\n",
      "Processing: 17__ARTICLE_17000_APPRENTICES.txt\n",
      "Processing: 18__ARTICLE_18000_HIRING_AND_TRANSFER_OF_WORKERS.txt\n",
      "Processing: 19__ARTICLE_19000_CREW_SIZE_SUPERVISION_AND_FOREPERSONS.txt\n",
      "Processing: 20__ARTICLE_20000_LOCKOUT_AND_WORK_STOPPAGE.txt\n",
      "Processing: 21__ARTICLE_21000_TOOL_ROOMS_AND_TOOLS.txt\n",
      "Processing: 22__ARTICLE_22000_MANAGEMENT_CLAUSE.txt\n",
      "Processing: 23__ARTICLE_23000_TRADE_CO-OPERATION.txt\n",
      "Processing: 24__ARTICLE_24000_DURATION_AND_TERMINATION_OF_AGREEMENT.txt\n",
      "Processing: 25__ARTICLE_25000_ELECTRONIC_SIGNATURE.txt\n",
      "Processing: 26__APPENDIX_A.txt\n",
      "Processing: 27_SIGNATORY_UNIONS.txt\n",
      "Processing: 28__APPENDIX_B.txt\n",
      "Processing: 29__SIGNATORY_EMPLOYERS.txt\n",
      "Processing: 30_NATIONAL_MAINTENANCE_AGREEMENT_FOR_ALBERTA_HOURS_OF_WORK_PROVISIONSSUNCOR_REFINE.txt\n",
      "Processing: 31__APPENDIX_D_-_APPROVED_WORK_WEEK_ALTERATIONS.txt\n",
      "Processing: 32__980_WORK_SCHEDULE.txt\n",
      "Processing: 33__IOL_STRATHCONA.txt\n",
      "Processing: 34__APPENDIX_E.txt\n",
      "Processing: 35__APPENDIX_F.txt\n",
      "Processing: 36_POLICY.txt\n",
      "Processing: 37__ALBERTA_BEREAVEMENT_PROTOCOL.txt\n",
      "Processing: 38_THE_NATIONAL_MAINTENANCE_COUNCIL_FOR_CANADA_BEREAVEMENT_PROTOCOL.txt\n",
      "Processing: 39__ADMINISTRATION_FUND_A.txt\n",
      "Processing: 40__NATIONAL_MAINTENANCE_COUNCIL_FOR_CANADA.txt\n",
      "Processing: 41__ADMINISTRATION_FUND_B.txt\n",
      "Processing: 42__NATIONAL_MAINTENANCE_COUNCIL_FOR_CANADA.txt\n",
      "Processing: 43__TRADE_APPENDIX.txt\n",
      "Processing: 01_10_DEFINITIONS.txt\n",
      "Processing: 02_20_CONTRACT_DOCUMENTS_AND_PRECEDENCE.txt\n",
      "Processing: 03_30_TERM.txt\n",
      "Processing: 04_40_PROVISION_OF_WORK.txt\n",
      "Processing: 05_50_CHANGES_TO_THE_WORK.txt\n",
      "Processing: 06_60_TITLE_AND_RISK_OF_LOSS.txt\n",
      "Processing: 07_70_SUPPLIER_REPRESENTATIONS.txt\n",
      "Processing: 08_80_OWNERSHIP_OF_DOCUMENTS.txt\n",
      "Processing: 09_90_SUPPLIERS_EQUIPMENT_AND_MATERIALS.txt\n",
      "Processing: 10_100_LIENS.txt\n",
      "Processing: 11_110_INSURANCE.txt\n",
      "Processing: 12_120_INSPECTION_AND_EXPEDITING.txt\n",
      "Processing: 13_130_LIABILITY_AND_INDEMNITY.txt\n",
      "Processing: 14_140_WARRANTY.txt\n",
      "Processing: 15_150_TERMINATION.txt\n",
      "Processing: 16_160_SUSPENSION.txt\n",
      "Processing: 17_170_INTELLECTUAL_PROPERTY.txt\n",
      "Processing: 18_180_CONFIDENTIALITY_AND_PUBLICITY.txt\n",
      "Processing: 19_190_SUBCONTRACTS.txt\n",
      "Processing: 20_200_EXCUSED_NON-PERFORMANCE.txt\n",
      "Processing: 21_210_ETHICAL_CONDUCT.txt\n",
      "Processing: 22_220_DISPUTE_RESOLUTION_PROCEDURE.txt\n",
      "Processing: 23_230_MATERIALS_MANAGEMENT_amp_HAZARDOUS_SUBSTANCES.txt\n",
      "Processing: 24_240_PRICING_INVOICING_AND_PAYMENT.txt\n",
      "Processing: 25_250_TAXES.txt\n",
      "Processing: 26_260_WITHHOLDING_OF_PAYMENT.txt\n",
      "Processing: 27_270_SET-OFF_AND_BACKCHARGES.txt\n",
      "Processing: 28_280_RIGHT_TO_AUDIT.txt\n",
      "Processing: 29_290_DELIVERY.txt\n",
      "Processing: 30_300_KEY_PERSONNEL.txt\n",
      "Processing: 31_310_PERFORMANCE_SECURITY.txt\n",
      "Processing: 32_320_SECURITY.txt\n",
      "Processing: 33_330_GOVERNING_LAW.txt\n",
      "Processing: 34_340_NOTICES.txt\n",
      "Processing: 35_350_MISCELLANEOUS.txt\n",
      "Processing: 01__ARTICLE_ONE_-_RECOGNITION.txt\n",
      "Processing: 02__ARTICLE_TWO_-_SUB-CONTRACTORS_CLAUSE.txt\n",
      "Processing: 03__ARTICLE_THREE_-_TRADE_OR_WORK_JURISDICTION.txt\n",
      "Processing: 04__ARTICLE_FOUR_-_SCOPE_OF_AGREEMENT.txt\n",
      "Processing: 05__ARTICLE_FIVE_-_MANAGEMENTS_RIGHTS.txt\n",
      "Processing: 06__ARTICLE_SIX_-_UNION_SECURITY.txt\n",
      "Processing: 07__ARTICLE_SEVEN_-_HIRING.txt\n",
      "Processing: 08_ARTICLE_EIGHT_-_WAGES_BENEFITS_amp_MONETARY_CONDITIONS.txt\n",
      "Processing: 09__ARTICLE_NINE_-_HEALTH_AND_WELFARE_TRUST_FUNDS.txt\n",
      "Processing: 10__ARTICLE_TEN_-_PENSION_TRUST_FUNDS.txt\n",
      "Processing: 11__ARTICLE_ELEVEN_-_JOINT_EDUCATIONAL_TRUST_FUNDS.txt\n",
      "Processing: 12__ARTICLE_TWELVE_-_SUPPLEMENTARY_BENEFIT_TRUST_FUNDS.txt\n",
      "Processing: 13__ARTICLE_THIRTEEN_-_CUSTOMER_WELDING_TESTS.txt\n",
      "Processing: 14__ARTICLE_FOURTEEN_-_RECOGNIZED_HOLIDAYS_AND_VACATION_WITH_PAY.txt\n",
      "Processing: 15__ARTICLE_FIFTEEN_-_GENERAL_CONDITIONS.txt\n",
      "Processing: 16__ARTICLE_SIXTEEN_-_HOURS_OF_WORK.txt\n",
      "Processing: 17__ARTICLE_SEVENTEEN_-_COMPRESSED_WORK_WEEK.txt\n",
      "Processing: 18__ARTICLE_EIGHTEEN_-_CALL_OUTS.txt\n",
      "Processing: 19__ARTICLE_NINETEEN_-_SHIFT_WORK.txt\n",
      "Processing: 20__ARTICLE_TWENTY_-_OVERTIME_MEALS.txt\n",
      "Processing: 21__ARTICLE_TWENTY-ONE_-_SHOW_UP_TIME.txt\n",
      "Processing: 22__ARTICLE_TWENTY-TWO_-_MATERIAL_HANDLING_AND_FABRICATION.txt\n",
      "Processing: 23__ARTICLE_TWENTY-THREE_-_LOCAL_RESIDENTS.txt\n",
      "Processing: 24__ARTICLE_TWENTY-FOUR_-_TOOLS_AND_PROTECTIVE_CLOTHING.txt\n",
      "Processing: 25__ARTICLE_TWENTY-FIVE_-_FOREMEN.txt\n",
      "Processing: 26__ARTICLE_TWENTY-SIX_-_GRIEVANCE_AND_ARBITRATION_PROCEDURE.txt\n",
      "Processing: 27__ARTICLE_TWENTY-SEVEN_-_JOINT_CONFERENCE_BOARD.txt\n",
      "Processing: 28__ARTICLE_TWENTY-EIGHT_-_SAVING_CLAUSE.txt\n",
      "Processing: 29__ARTICLE_TWENTY-NINE_-_TRAVEL_TRAVEL_ALLOWANCE_TRANSPORTATION_AND_ACCOMMODATION.txt\n",
      "Processing: 30__ARTICLE_THIRTY_-_ENABLING_CLAUSE.txt\n",
      "Processing: 31__ARTICLE_THIRTY-ONE_-_EMPLOYER_ASSOCIATION_FUNDS.txt\n",
      "Processing: 32_ARTICLE_THIRTY-TWO_-_INDUSTRY_STANDARD_IMPROVEMENT_TRUST_FUND_Mechanical_Members.txt\n",
      "Processing: 33__ARTICLE_THIRTY-THREE_-_DURATION.txt\n",
      "Processing: 34_Letter_of_Understanding_Re_Referral_for_Case_Managed_Aftercare.txt\n",
      "Processing: 35_Letter_of_Understanding_re_Rapid_Site_Access_Program.txt\n",
      "Processing: 36_Letter_of_Understanding_re_Special_Project_Needs_Agreements_SPNA.txt\n",
      "Processing: 37_Letter_of_Understanding_Re_Training_Initiatives.txt\n",
      "Processing: 38_Memorandum_of_Commitment_RE_BTUREO_Industry_Committee.txt\n",
      "Processing: 39_Letter_of_Intent_RE_JOINT_INDEPENDENT_ALCOHOL_amp_DRUG_PROGRAM.txt\n",
      "Classified 155 chunks into /Users/swathi.gnanasekar/Documents/Vista_Vu_Project/Phase 1/Docling_Tweak/Categorized/result_version 1.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BASE_DIR = Path(\"/Users/swathi.gnanasekar/Documents/Vista_Vu_Project/Phase 1/Docling_Tweak/CHUNK_NEW\")\n",
    "OUTPUT_PATH = Path(\"/Users/swathi.gnanasekar/Documents/Vista_Vu_Project/Phase 1/Docling_Tweak/Categorized/result_version 0.json\")\n",
    "\n",
    "SYSTEM_SUFFIX = \"\\n\\nOnly return the JSON object described under 'Required Output Format'. Do not add code fences or extra text.\"\n",
    "\n",
    "# 2. INITIALIZE LANGFUSE\n",
    "langfuse = Langfuse()\n",
    "\n",
    "# 3. INITIALIZE MODEL\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    temperature=0,\n",
    "    convert_system_message_to_human=True,\n",
    ")\n",
    "\n",
    "def extract_json(content) -> dict:\n",
    "    \"\"\"Normalize model content then parse JSON.\"\"\"\n",
    "    if isinstance(content, list):\n",
    "        text_parts = []\n",
    "        for part in content:\n",
    "            if isinstance(part, dict):\n",
    "                text_parts.append(part.get(\"text\", \"\"))\n",
    "            else:\n",
    "                text_parts.append(str(part))\n",
    "        content = \" \".join(text_parts).strip()\n",
    "    if not isinstance(content, str):\n",
    "        content = str(content)\n",
    "\n",
    "    try:\n",
    "        return json.loads(content)\n",
    "    except json.JSONDecodeError:\n",
    "        match = re.search(r\"\\{.*\\}\", content, re.DOTALL)\n",
    "        if match:\n",
    "            return json.loads(match.group())\n",
    "        raise\n",
    "\n",
    "def classify_chunk(chunk_text: str, chunk_name: str) -> dict:\n",
    "    # 4. FETCH PROMPT FROM LANGFUSE\n",
    "    # This pulls the latest version. If you edit the prompt in the UI, \n",
    "    # this line gets the new text automatically.\n",
    "    langfuse_prompt = langfuse.get_prompt(\"chunk-classification-system\")\n",
    "    \n",
    "    # We compile the prompt (if you had variables like {{variable}}, this handles them)\n",
    "    # Since your system prompt is static text, .compile() just returns the string.\n",
    "    system_prompt_text = langfuse_prompt.compile()\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt_text + SYSTEM_SUFFIX),\n",
    "        HumanMessage(content=f\"Chunk name: {chunk_name}\\n\\nChunk text:\\n{chunk_text}\"),\n",
    "    ]\n",
    "    \n",
    "    # 5. INITIALIZE CALLBACK HANDLER\n",
    "    # This handler watches the execution and sends data to your localhost Langfuse\n",
    "    langfuse_handler = CallbackHandler()\n",
    "\n",
    "    # 6. INVOKE WITH CALLBACK AND PROMPT TRACKING\n",
    "    # We pass the handler to 'callbacks'.\n",
    "    # We verify the trace is linked to the prompt by passing the prompt object to the handler logic if needed,\n",
    "    # but strictly speaking, LangChain integration traces the *execution*.\n",
    "    # To strictly link the \"Prompt Version\" in the UI, we usually use langfuse_prompt.get_langchain_prompt()\n",
    "    # BUT for your custom message structure, the handler will log the input/output perfectly.\n",
    "    response = model.invoke(\n",
    "        messages, \n",
    "        config={\"callbacks\": [langfuse_handler]}\n",
    "    )\n",
    "    \n",
    "    return extract_json(response.content)\n",
    "\n",
    "# ... (The rest of your file loop logic remains the same) ...\n",
    "\n",
    "results = []\n",
    "txt_files = sorted(BASE_DIR.glob(\"**/*.txt\"))\n",
    "\n",
    "# Processing loop\n",
    "for idx, path in enumerate(txt_files, start=1):\n",
    "    chunk_text = path.read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n",
    "    if not chunk_text:\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing: {path.name}\")\n",
    "    \n",
    "    try:\n",
    "        classification = classify_chunk(chunk_text, path.name)\n",
    "    except Exception as exc:\n",
    "        classification = {\n",
    "            \"Contract Section Type\": \"ERROR\",\n",
    "            \"Second Level\": \"\",\n",
    "            \"Third Level\": \"\",\n",
    "            \"error\": str(exc),\n",
    "        }\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"chunk_file\": str(path.relative_to(BASE_DIR)),\n",
    "            \"classification\": classification,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Small delay\n",
    "    time.sleep(0.2)\n",
    "\n",
    "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Classified {len(results)} chunks into {OUTPUT_PATH}\")\n",
    " \n",
    "# Ensure all traces are sent before exiting\n",
    "langfuse.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9850a956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf9bda7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vistavu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
