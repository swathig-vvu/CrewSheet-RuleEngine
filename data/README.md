# Project Data

This directory contains all the datasets used and generated by the VistaVu Rule Extraction Engine.

## Directory Structure

### 1. `contracts_initial/`
**Content**: Raw Source Documents.
*   Contains the original PDF files for the labour contracts (e.g., NMA Alberta, Boilermakers Agreement).
*   Organized by category (Customer, Union Agreements, etc.).

### 2. `docling_conversion/`
**Content**: Intermediate Processing Artifacts.
*   **Markdown Files**: The direct output of the PDF-to-Markdown conversion process.
*   **JSON Files**: Structured representations of the documents from Docling.
*   **CHUNK_NEW/**: The output of the chunking process. Contains thousands of small `.txt` files, each representing a single section of a contract.

### 3. `db/`
**Content**: Structured Database.
*   `labour_rules.db`: An SQLite database containing the text chunks and their classification metadata. This is the source of truth for the rule extraction engine.

### 4. `rules/`
**Content**: Final Outputs.
*   `extracted_rules/`: Contains the JSON files output by the Gemini rule extraction process.
    *   `labour_rules_extracted.json`: All raw rules extracted.
    *   `LABOR_WORKED_TIME_RULES.json`: Filtered rules specifically related to working hours and time.
    *   `failed_chunks_detailed.json`: A log of any chunks that failed processing, for debugging.

## Note on Git Tracking
This entire `data/` directory is **tracked** in the git repository to facilitate collaboration. Please be mindful of file sizes when adding new PDFs.
